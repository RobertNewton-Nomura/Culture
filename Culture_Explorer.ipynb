{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a79157",
   "metadata": {},
   "source": [
    "# Culture Explorer: World & European Values Survey Toolkit\n",
    "\n",
    "This notebook delivers an end-to-end cultural analytics workspace that can be executed in Google Colab. It combines a static geopandas/matplotlib map (no JavaScript required), comparison dashboards, a pure Python helper layer, and OpenAI-powered cultural insights to satisfy the requirements outlined in the project brief.\n",
    "\n",
    "**What you can do here:**\n",
    "\n",
    "* Explore World Values Survey (WVS) and European Values Survey (EVS) indicators on a global map.\n",
    "* Build comparison matrices for arbitrary sets of countries, fine-tuning the weight of individual survey questions through interactive controls.\n",
    "* Capture retrospective and future survey measurements directly inside the notebook or via the helper service methods.\n",
    "* Create family or team groups, collect their survey responses, and match their profiles to the closest countries.\n",
    "* Plug in an OpenAI API key to obtain reusable cultural briefings, collaboration advice, and conflict mediation guidance from the survey scores.\n",
    "\n",
    "> **Data**: A small representative CSV is bundled inside the repository for offline execution. In a Colab environment you can replace it with the official WVS/EVS extracts placed under the `data/` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bc13e8",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "The following cell installs all runtime dependencies. Comment it out if you already manage packages elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322a696",
   "metadata": {},
   "source": [
    "## 2. Imports and global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0ce0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:12.757478Z",
     "iopub.status.busy": "2025-10-02T16:16:12.755968Z",
     "iopub.status.idle": "2025-10-02T16:16:15.644513Z",
     "shell.execute_reply": "2025-10-02T16:16:15.640361Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "from itertools import groupby\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except ImportError:\n",
    "    gpd = None\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except ImportError:\n",
    "    widgets = None\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "try:\n",
    "    import pdfplumber  # type: ignore\n",
    "except ImportError:  # pragma: no cover - optional dependency for parsing PDF catalogues\n",
    "    pdfplumber = None\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "COUNTRY_PATH = DATA_DIR / \"country.csv\"\n",
    "LOCATION_PATH = DATA_DIR / \"long_lat.csv\"\n",
    "COLUMN_SNAPSHOT_PATH = DATA_DIR / \"column_snapshot.json\"\n",
    "QUESTION_INDEX_PATH = DATA_DIR / \"question_index.json\"\n",
    "CACHE_PATH = DATA_DIR / \"openai_response_cache.json\"\n",
    "QUESTION_CODE_PATTERN = re.compile(r'^([A-Za-z]+\\d+[A-Za-z0-9-]*)$')\n",
    "\n",
    "def _sanitise_column_name(name: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9]', '', str(name).lower())\n",
    "\n",
    "def derive_standard_column_mapping(columns: List[str], entity_type: str) -> Dict[str, str]:\n",
    "    patterns = {\n",
    "        'Country': ['country', 'countryname', 'nation'],\n",
    "        'ISO3': ['iso3', 'iso'],\n",
    "        'Latitude': ['latitude', 'lat'],\n",
    "        'Longitude': ['longitude', 'long', 'lng'],\n",
    "        'Source': ['source', 'dataset', 'survey'],\n",
    "        'Year': ['year'],\n",
    "    }\n",
    "    if entity_type == 'location':\n",
    "        patterns['Area'] = ['area', 'region', 'state', 'province', 'location', 'admin', 'adminname']\n",
    "    mapping: Dict[str, str] = {}\n",
    "    sanitised = {col: _sanitise_column_name(col) for col in columns}\n",
    "    for target, keywords in patterns.items():\n",
    "        match = None\n",
    "        # exact match first\n",
    "        for keyword in keywords:\n",
    "            for col, norm in sanitised.items():\n",
    "                if norm == keyword:\n",
    "                    match = col\n",
    "                    break\n",
    "            if match:\n",
    "                break\n",
    "        if not match:\n",
    "            for keyword in keywords:\n",
    "                for col, norm in sanitised.items():\n",
    "                    if norm.endswith(keyword) or keyword in norm:\n",
    "                        match = col\n",
    "                        break\n",
    "                if match:\n",
    "                    break\n",
    "        if match and target not in mapping.values():\n",
    "            mapping[match] = target\n",
    "    return mapping\n",
    "\n",
    "def _looks_like_pdf(path: Path) -> bool:\n",
    "    try:\n",
    "        with path.open(\"rb\") as handle:\n",
    "            return handle.read(4) == b\"%PDF\"\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def find_question_catalogue(data_dir: Path) -> Optional[Path]:\n",
    "    search_roots = [\n",
    "        data_dir,\n",
    "        Path(\"questions\"),\n",
    "        Path.cwd(),\n",
    "    ]\n",
    "    candidates: List[Path] = []\n",
    "    for root in search_roots:\n",
    "        if root.is_file():\n",
    "            if root.suffix.lower() == \".pdf\" or _looks_like_pdf(root):\n",
    "                candidates.append(root.resolve())\n",
    "            continue\n",
    "        if not root.exists() or not root.is_dir():\n",
    "            continue\n",
    "        for candidate in root.glob(\"*.pdf\"):\n",
    "            if candidate.is_file():\n",
    "                candidates.append(candidate.resolve())\n",
    "        for candidate in root.glob(\"*\"):\n",
    "            if candidate.is_file() and _looks_like_pdf(candidate):\n",
    "                candidates.append(candidate.resolve())\n",
    "    unique_candidates: List[Path] = []\n",
    "    seen: set[Path] = set()\n",
    "    for candidate in candidates:\n",
    "        if candidate not in seen:\n",
    "            unique_candidates.append(candidate)\n",
    "            seen.add(candidate)\n",
    "    prioritised = [path for path in unique_candidates if \"question\" in path.name.lower()]\n",
    "    return prioritised[0] if prioritised else (unique_candidates[0] if unique_candidates else None)\n",
    "\n",
    "def extract_official_question_titles(pdf_path: Path) -> Dict[str, str]:\n",
    "    if pdfplumber is None:\n",
    "        return {}\n",
    "    question_titles: Dict[str, str] = {}\n",
    "    try:\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "                if not words:\n",
    "                    continue\n",
    "                words.sort(key=lambda w: (round(w['top'], 1), w['x0']))\n",
    "                for _, group in groupby(words, key=lambda w: round(w['top'], 1)):\n",
    "                    row = sorted(group, key=lambda w: w['x0'])\n",
    "                    idx = 0\n",
    "                    while idx < len(row):\n",
    "                        raw_code = row[idx]['text'].strip()\n",
    "                        candidate = re.sub(r'[\\s.:]+$', '', raw_code)\n",
    "                        if QUESTION_CODE_PATTERN.match(candidate):\n",
    "                            code = candidate\n",
    "                            idx += 1\n",
    "                            text_parts: List[str] = []\n",
    "                            while idx < len(row):\n",
    "                                next_text = row[idx]['text'].strip()\n",
    "                                cleaned = re.sub(r'[\\s.:]+$', '', next_text)\n",
    "                                if QUESTION_CODE_PATTERN.match(cleaned):\n",
    "                                    break\n",
    "                                text_parts.append(next_text)\n",
    "                                idx += 1\n",
    "                            question = ' '.join(text_parts).strip()\n",
    "                            question = re.sub(r'(?:\\s+\\d+)+$', '', question).strip()\n",
    "                            question = re.sub(r'\\s{2,}', ' ', question)\n",
    "                            if question and code not in question_titles:\n",
    "                                question_titles[code] = question\n",
    "                        else:\n",
    "                            idx += 1\n",
    "    except Exception as exc:  # pragma: no cover - parsing quality depends on PDF formatting\n",
    "        display(Markdown(f'⚠️ **Warning:** Unable to parse `{pdf_path.name}` for question titles ({exc}).'))\n",
    "    return question_titles\n",
    "\n",
    "def normalise_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(col).strip() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_required_columns(df: pd.DataFrame, entity_type: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    mapping = derive_standard_column_mapping(df.columns.tolist(), entity_type)\n",
    "    if mapping:\n",
    "        df = df.rename(columns=mapping)\n",
    "    if 'Source' not in df.columns:\n",
    "        df['Source'] = 'World Values Survey'\n",
    "    if 'Year' not in df.columns:\n",
    "        df['Year'] = pd.Timestamp.today().year\n",
    "    if 'ISO3' not in df.columns:\n",
    "        if 'Country' in df.columns:\n",
    "            df['ISO3'] = df['Country'].astype(str).str[:3].str.upper()\n",
    "        else:\n",
    "            df['ISO3'] = 'UNK'\n",
    "    if entity_type == 'location' and 'Area' not in df.columns:\n",
    "        fallback = None\n",
    "        for candidate in ['Location', 'Region', 'State', 'Province']:\n",
    "            if candidate in df.columns:\n",
    "                fallback = candidate\n",
    "                break\n",
    "        df['Area'] = df[fallback] if fallback else 'Unknown'\n",
    "    return df\n",
    "\n",
    "def summarise_spreadsheet(path: Path, titles: Dict[str, str]) -> Dict[str, object]:\n",
    "    if not path.exists():\n",
    "        return {\"columns\": [], \"first_row\": {}, \"question_index\": []}\n",
    "    try:\n",
    "        raw = pd.read_csv(path)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return {\"columns\": [], \"first_row\": {}, \"question_index\": []}\n",
    "    raw = normalise_headers(raw)\n",
    "    columns = [str(col).strip() for col in raw.columns]\n",
    "    raw.columns = columns\n",
    "    if raw.empty:\n",
    "        first_row = {}\n",
    "    else:\n",
    "        sample = raw.iloc[0]\n",
    "        def _convert(value: object) -> object:\n",
    "            if pd.isna(value):\n",
    "                return None\n",
    "            if isinstance(value, np.generic):\n",
    "                try:\n",
    "                    return value.item()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if hasattr(value, \"item\"):\n",
    "                try:\n",
    "                    return value.item()\n",
    "                except Exception:\n",
    "                    return value\n",
    "            return value\n",
    "        first_row = {col: _convert(sample[col]) for col in columns}\n",
    "    question_index = [\n",
    "        {\n",
    "            \"column_number\": idx + 1,\n",
    "            \"code\": col,\n",
    "            \"title\": titles.get(col),\n",
    "            \"found_in_pdf\": col in titles,\n",
    "        }\n",
    "        for idx, col in enumerate(columns)\n",
    "        if QUESTION_CODE_PATTERN.match(str(col))\n",
    "    ]\n",
    "    return {\"columns\": columns, \"first_row\": first_row, \"question_index\": question_index}\n",
    "\n",
    "def load_culture_frame(path: Path, entity_type: str, titles: Dict[str, str]) -> pd.DataFrame:\n",
    "    base_columns = [\n",
    "        'Country',\n",
    "        'ParentCountry',\n",
    "        'ISO3',\n",
    "        'Latitude',\n",
    "        'Longitude',\n",
    "        'Year',\n",
    "        'Source',\n",
    "        'QuestionGroup',\n",
    "        'Question',\n",
    "        'QuestionCode',\n",
    "        'Score',\n",
    "        'EntityType',\n",
    "    ]\n",
    "    if entity_type == 'location':\n",
    "        base_columns.append('Area')\n",
    "\n",
    "    def _empty_frame() -> pd.DataFrame:\n",
    "        frame = pd.DataFrame(columns=base_columns)\n",
    "        frame['EntityType'] = entity_type\n",
    "        return frame\n",
    "\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        return _empty_frame()\n",
    "    try:\n",
    "        raw = pd.read_csv(path)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return _empty_frame()\n",
    "    if raw.empty:\n",
    "        return _empty_frame()\n",
    "    raw = normalise_headers(raw)\n",
    "    raw = ensure_required_columns(raw, entity_type)\n",
    "\n",
    "    numeric_columns = ['Latitude', 'Longitude', 'Year']\n",
    "    for column in numeric_columns:\n",
    "        if column in raw.columns:\n",
    "            raw[column] = pd.to_numeric(raw[column], errors='coerce')\n",
    "    raw = raw.dropna(subset=[col for col in ['Latitude', 'Longitude', 'Year'] if col in raw.columns])\n",
    "    if raw.empty:\n",
    "        return _empty_frame()\n",
    "    raw['Year'] = raw['Year'].astype(int)\n",
    "\n",
    "    question_columns = [col for col in raw.columns if QUESTION_CODE_PATTERN.match(str(col))]\n",
    "    if not question_columns:\n",
    "        if entity_type == 'country':\n",
    "            raise ValueError(\n",
    "                f\"No survey question columns detected in `{path.name}`. Columns must use official codes like 'A001'.\"\n",
    "            )\n",
    "        return _empty_frame()\n",
    "    id_columns = [col for col in raw.columns if col not in question_columns]\n",
    "\n",
    "    melted = raw.melt(id_vars=id_columns, value_vars=question_columns, var_name='QuestionCode', value_name='Score')\n",
    "    melted['Score'] = pd.to_numeric(melted['Score'], errors='coerce')\n",
    "    melted = melted.dropna(subset=['Score', 'Latitude', 'Longitude', 'Year'])\n",
    "    if melted.empty:\n",
    "        return _empty_frame()\n",
    "    melted['Score'] = melted['Score'].astype(float)\n",
    "\n",
    "    melted['QuestionCode'] = melted['QuestionCode'].astype(str)\n",
    "    melted['QuestionGroup'] = (\n",
    "        melted['QuestionCode'].str.extract(r'^([A-Za-z]+)', expand=False).str.upper().fillna('GENERAL')\n",
    "    )\n",
    "    melted['Question'] = [titles.get(code, code) for code in melted['QuestionCode']]\n",
    "    melted['EntityType'] = entity_type\n",
    "\n",
    "    if entity_type == 'country':\n",
    "        melted['ParentCountry'] = melted['Country'].astype(str)\n",
    "        melted['DisplayName'] = melted['Country'].astype(str)\n",
    "    else:\n",
    "        area_column = None\n",
    "        for candidate in ['Area', 'Region', 'Location', 'State', 'Province']:\n",
    "            if candidate in melted.columns:\n",
    "                area_column = candidate\n",
    "                break\n",
    "        if area_column is None:\n",
    "            melted['Area'] = melted['Country'].astype(str)\n",
    "            area_column = 'Area'\n",
    "        melted['ParentCountry'] = melted['Country'].astype(str)\n",
    "        melted['DisplayName'] = (\n",
    "            melted[area_column].astype(str).str.strip() + ' — ' + melted['ParentCountry'].astype(str).str.strip()\n",
    "        )\n",
    "        melted['Country'] = melted['DisplayName']\n",
    "\n",
    "    if 'ISO3' not in melted.columns:\n",
    "        base = melted.get('ParentCountry', melted.get('Country'))\n",
    "        melted['ISO3'] = base.astype(str).str[:3].str.upper() if base is not None else 'UNK'\n",
    "\n",
    "    columns = [\n",
    "        'Country',\n",
    "        'ParentCountry',\n",
    "        'ISO3',\n",
    "        'Latitude',\n",
    "        'Longitude',\n",
    "        'Year',\n",
    "        'Source',\n",
    "        'QuestionGroup',\n",
    "        'Question',\n",
    "        'QuestionCode',\n",
    "        'Score',\n",
    "        'EntityType',\n",
    "    ]\n",
    "    if 'Area' in melted.columns:\n",
    "        columns.append('Area')\n",
    "\n",
    "    melted = melted[[col for col in columns if col in melted.columns]].copy()\n",
    "    melted['Country'] = melted['Country'].astype(str)\n",
    "    melted['ParentCountry'] = melted['ParentCountry'].astype(str)\n",
    "    return melted.reset_index(drop=True)\n",
    "\n",
    "QUESTION_PDF = find_question_catalogue(DATA_DIR)\n",
    "OFFICIAL_TITLES = extract_official_question_titles(QUESTION_PDF) if QUESTION_PDF else {}\n",
    "\n",
    "if not CACHE_PATH.exists():\n",
    "    CACHE_PATH.write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "country_records = load_culture_frame(COUNTRY_PATH, \"country\", OFFICIAL_TITLES)\n",
    "location_records = load_culture_frame(LOCATION_PATH, \"location\", OFFICIAL_TITLES)\n",
    "\n",
    "country_summary = summarise_spreadsheet(COUNTRY_PATH, OFFICIAL_TITLES)\n",
    "location_summary = summarise_spreadsheet(LOCATION_PATH, OFFICIAL_TITLES)\n",
    "\n",
    "COLUMN_SUMMARIES = {\n",
    "    \"country\": {\"columns\": country_summary[\"columns\"], \"first_row\": country_summary[\"first_row\"]},\n",
    "    \"location\": {\"columns\": location_summary[\"columns\"], \"first_row\": location_summary[\"first_row\"]},\n",
    "}\n",
    "QUESTION_INDEX = {\n",
    "    \"country\": country_summary[\"question_index\"],\n",
    "    \"location\": location_summary[\"question_index\"],\n",
    "}\n",
    "\n",
    "COLUMN_SNAPSHOT_PATH.write_text(\n",
    "    json.dumps(COLUMN_SUMMARIES, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "QUESTION_INDEX_PATH.write_text(\n",
    "    json.dumps(QUESTION_INDEX, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                f\"Loaded **{len(country_records)}** country-level responses from `{COUNTRY_PATH.name}` covering **{country_records['Country'].nunique()}** countries.\",\n",
    "                f\"Loaded **{len(location_records)}** local observations from `{LOCATION_PATH.name}` across **{location_records['Country'].nunique()}** areas.\",\n",
    "                \"Mapped question codes to official titles.\" if OFFICIAL_TITLES else \"Using question codes directly (no PDF detected).\",\n",
    "                f\"Captured first-row snapshots in `{COLUMN_SNAPSHOT_PATH.name}` and question index in `{QUESTION_INDEX_PATH.name}`.\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "if country_summary[\"columns\"]:\n",
    "    display(Markdown(\"**Country data — first row preview (first 10 columns):**\"))\n",
    "    preview_cols = country_summary[\"columns\"][:10]\n",
    "    display(pd.DataFrame([country_summary[\"first_row\"]])[preview_cols])\n",
    "\n",
    "if QUESTION_INDEX[\"country\"]:\n",
    "    display(Markdown(\"**Sample question index mapping (country data):**\"))\n",
    "    display(pd.DataFrame(QUESTION_INDEX[\"country\"]).head())\n",
    "\n",
    "country_records.head()\n",
    "location_records.head()\n",
    "QuestionKey = Tuple[str, str]\n",
    "\n",
    "\n",
    "class CulturalDataset:\n",
    "    \"\"\"Convenience wrapper around country and location survey records.\"\"\"\n",
    "\n",
    "    required_columns = {\n",
    "        \"Country\",\n",
    "        \"ParentCountry\",\n",
    "        \"ISO3\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Year\",\n",
    "        \"Source\",\n",
    "        \"QuestionGroup\",\n",
    "        \"Question\",\n",
    "        \"QuestionCode\",\n",
    "        \"Score\",\n",
    "        \"EntityType\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, country_df: pd.DataFrame, location_df: pd.DataFrame):\n",
    "        self._validate_input(\"country\", country_df)\n",
    "        self._validate_input(\"location\", location_df)\n",
    "        self.country_data = country_df.copy()\n",
    "        self.location_data = location_df.copy()\n",
    "        self.data = self.country_data.copy()\n",
    "        self.base_countries = sorted(self.country_data[\"Country\"].unique())\n",
    "        self.location_lookup = {\n",
    "            name: group.copy() for name, group in self.location_data.groupby(\"Country\")\n",
    "        }\n",
    "        self.question_lookup: Dict[QuestionKey, str] = {}\n",
    "        combined = pd.concat([self.country_data, self.location_data], ignore_index=True)\n",
    "        for _, row in (\n",
    "            combined[[\"QuestionGroup\", \"Question\", \"QuestionCode\"]]\n",
    "            .drop_duplicates()\n",
    "            .iterrows()\n",
    "        ):\n",
    "            self.question_lookup[(row[\"QuestionGroup\"], row[\"Question\"])] = row[\"QuestionCode\"]\n",
    "        self._refresh_metadata()\n",
    "\n",
    "    def _validate_input(self, label: str, frame: pd.DataFrame) -> None:\n",
    "        missing = self.required_columns - set(frame.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"{label.capitalize()} data is missing required columns: {missing}\")\n",
    "\n",
    "    def _refresh_metadata(self) -> None:\n",
    "        records: List[pd.DataFrame] = []\n",
    "        for frame in [self.country_data, self.location_data]:\n",
    "            if frame.empty:\n",
    "                continue\n",
    "            meta = (\n",
    "                frame.groupby(\"Country\")\n",
    "                .agg(\n",
    "                    Latitude=(\"Latitude\", \"mean\"),\n",
    "                    Longitude=(\"Longitude\", \"mean\"),\n",
    "                    ParentCountry=(\"ParentCountry\", \"first\"),\n",
    "                    EntityType=(\"EntityType\", \"first\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "            records.append(meta)\n",
    "        if records:\n",
    "            combined = (\n",
    "                pd.concat(records, ignore_index=True)\n",
    "                .drop_duplicates(\"Country\", keep=\"first\")\n",
    "            )\n",
    "            self.metadata = {row[\"Country\"]: row.to_dict() for _, row in combined.iterrows()}\n",
    "        else:\n",
    "            self.metadata = {}\n",
    "        self.available_entities = sorted(self.data[\"Country\"].unique())\n",
    "\n",
    "    def get_countries(self) -> List[str]:\n",
    "        return self.available_entities.copy()\n",
    "\n",
    "    def get_locations(self) -> List[str]:\n",
    "        return sorted(self.location_data[\"Country\"].unique())\n",
    "\n",
    "    def get_years(self) -> List[int]:\n",
    "        years = self.data[\"Year\"].unique().tolist()\n",
    "        return sorted(int(year) for year in years)\n",
    "\n",
    "    def get_group_questions(self) -> Dict[str, List[str]]:\n",
    "        grouped = self.data.groupby(\"QuestionGroup\")[\"Question\"].unique()\n",
    "        return {group: sorted(values.tolist()) for group, values in grouped.items()}\n",
    "\n",
    "\n",
    "    def get_sources(self) -> List[str]:\n",
    "        return sorted(self.data[\"Source\"].unique())\n",
    "\n",
    "    def get_question_code(self, group: str, question: str) -> str:\n",
    "        return self.question_lookup.get((group, question), question)\n",
    "\n",
    "    def filter_records(self, countries: Iterable[str], year: int) -> pd.DataFrame:\n",
    "        country_list = list(countries)\n",
    "        filtered = self.data[\n",
    "            self.data[\"Country\"].isin(country_list) & (self.data[\"Year\"] == year)\n",
    "        ]\n",
    "        if filtered.empty:\n",
    "            raise ValueError(\n",
    "                \"No records match the current filter. Try a different year or entity selection.\"\n",
    "            )\n",
    "        return filtered\n",
    "\n",
    "    def get_question_matrix(\n",
    "        self, countries: Iterable[str], year: int, group: Optional[str] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        subset = self.filter_records(countries, year)\n",
    "        if group and group != \"All Groups\":\n",
    "            subset = subset[subset[\"QuestionGroup\"] == group]\n",
    "        matrix = subset.pivot_table(index=\"Question\", columns=\"Country\", values=\"Score\")\n",
    "        return matrix.sort_index()\n",
    "\n",
    "    def compute_weighted_group_scores(\n",
    "        self, countries: Iterable[str], year: int, weights: Dict[QuestionKey, float]\n",
    "    ) -> pd.DataFrame:\n",
    "        subset = self.filter_records(countries, year).copy()\n",
    "        subset[\"Weight\"] = subset.apply(\n",
    "            lambda row: float(weights.get((row[\"QuestionGroup\"], row[\"Question\"]), 1.0)),\n",
    "            axis=1,\n",
    "        )\n",
    "        subset[\"WeightedScore\"] = subset[\"Score\"] * subset[\"Weight\"]\n",
    "\n",
    "        def safe_mean(group_df: pd.DataFrame) -> float:\n",
    "            weight_sum = group_df[\"Weight\"].sum()\n",
    "            if weight_sum == 0:\n",
    "                return float(\"nan\")\n",
    "            return group_df[\"WeightedScore\"].sum() / weight_sum\n",
    "\n",
    "        aggregated = (\n",
    "            subset.groupby([\"Country\", \"QuestionGroup\"], group_keys=False)\n",
    "            .apply(safe_mean, include_groups=False)\n",
    "            .unstack(\"QuestionGroup\")\n",
    "        )\n",
    "        return aggregated\n",
    "\n",
    "    def ensure_entity_available(self, entity_name: str, entity_type: str = \"country\") -> None:\n",
    "        if entity_name in self.available_entities:\n",
    "            return\n",
    "        if entity_type == \"location\":\n",
    "            if entity_name not in self.location_lookup:\n",
    "                raise ValueError(f\"Unknown location '{entity_name}'.\")\n",
    "            self.data = pd.concat([self.data, self.location_lookup[entity_name]], ignore_index=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown entity '{entity_name}'.\")\n",
    "        self._refresh_metadata()\n",
    "\n",
    "    def get_map_view(\n",
    "        self, entity_type: str, year: int, question_group: str, question: str\n",
    "    ) -> pd.DataFrame:\n",
    "        frame = self.country_data if entity_type == \"country\" else self.location_data\n",
    "        subset = frame[\n",
    "            (frame[\"Year\"] == year)\n",
    "            & (frame[\"QuestionGroup\"] == question_group)\n",
    "            & (frame[\"Question\"] == question)\n",
    "        ][[\n",
    "            \"Country\",\n",
    "            \"ParentCountry\",\n",
    "            \"ISO3\",\n",
    "            \"Latitude\",\n",
    "            \"Longitude\",\n",
    "            \"Year\",\n",
    "            \"Source\",\n",
    "            \"QuestionGroup\",\n",
    "            \"Question\",\n",
    "            \"Score\",\n",
    "            \"EntityType\",\n",
    "        ]]\n",
    "        return subset.reset_index(drop=True)\n",
    "\n",
    "    def get_entity_profile(\n",
    "        self, entity_name: str, entity_type: str, year: int\n",
    "    ) -> pd.DataFrame:\n",
    "        frame = self.country_data if entity_type == \"country\" else self.location_data\n",
    "        subset = frame[(frame[\"Country\"] == entity_name) & (frame[\"Year\"] == year)]\n",
    "        return subset[[\"QuestionGroup\", \"Question\", \"Score\"]]\n",
    "\n",
    "\n",
    "class GroupManager:\n",
    "    def __init__(self, dataset: CulturalDataset):\n",
    "        self.dataset = dataset\n",
    "        self.groups: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    def _validate_responses(self, responses: Dict[QuestionKey, float]) -> None:\n",
    "        valid_questions = self.dataset.get_group_questions()\n",
    "        for (group, question), value in responses.items():\n",
    "            if group not in valid_questions:\n",
    "                raise ValueError(f\"Unknown question group: {group}\")\n",
    "            if question not in valid_questions[group]:\n",
    "                raise ValueError(f\"Unknown question '{question}' for group '{group}'.\")\n",
    "            if not (0.0 <= value <= 1.0):\n",
    "                raise ValueError(\"Scores must be normalised between 0 and 1.\")\n",
    "\n",
    "    def add_member(\n",
    "        self,\n",
    "        group_name: str,\n",
    "        member_name: str,\n",
    "        responses: Dict[QuestionKey, float],\n",
    "        group_type: str = \"team\",\n",
    "        reference_year: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        group_name = group_name.strip()\n",
    "        if not group_name:\n",
    "            raise ValueError(\"Group name cannot be empty.\")\n",
    "        if not member_name.strip():\n",
    "            raise ValueError(\"Member name cannot be empty.\")\n",
    "        self._validate_responses(responses)\n",
    "        record = self.groups.setdefault(\n",
    "            group_name,\n",
    "            {\n",
    "                \"type\": group_type,\n",
    "                \"members\": {},\n",
    "                \"reference_year\": reference_year,\n",
    "            },\n",
    "        )\n",
    "        record[\"members\"][member_name] = responses\n",
    "        if reference_year is not None:\n",
    "            record[\"reference_year\"] = reference_year\n",
    "\n",
    "    def get_group_members(self, group_name: str) -> Dict[str, Dict[QuestionKey, float]]:\n",
    "        group = self.groups.get(group_name)\n",
    "        if not group:\n",
    "            raise KeyError(f\"Group '{group_name}' not found.\")\n",
    "        return group[\"members\"]  # type: ignore[return-value]\n",
    "\n",
    "    def compute_group_profile(self, group_name: str) -> pd.Series:\n",
    "        members = self.get_group_members(group_name)\n",
    "        if not members:\n",
    "            raise ValueError(\"Group has no members recorded yet.\")\n",
    "        df = pd.DataFrame(members).T\n",
    "        profile = df.mean(axis=0)\n",
    "        profile.name = group_name\n",
    "        return profile\n",
    "\n",
    "    def reference_year(self, group_name: str) -> Optional[int]:\n",
    "        group = self.groups.get(group_name)\n",
    "        return group.get(\"reference_year\") if group else None  # type: ignore[return-value]\n",
    "\n",
    "    def match_closest_country(\n",
    "        self, group_name: str, year: Optional[int] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        profile = self.compute_group_profile(group_name)\n",
    "        question_pairs = list(profile.index)\n",
    "        year = year or self.reference_year(group_name) or max(self.dataset.get_years())\n",
    "        entities = self.dataset.get_countries()\n",
    "        subset = self.dataset.filter_records(entities, year)\n",
    "        subset = subset[\n",
    "            subset.apply(\n",
    "                lambda row: (row[\"QuestionGroup\"], row[\"Question\"]) in question_pairs,\n",
    "                axis=1,\n",
    "            )\n",
    "        ]\n",
    "        matrix = subset.pivot_table(\n",
    "            index=\"Country\", columns=[\"QuestionGroup\", \"Question\"], values=\"Score\"\n",
    "        )\n",
    "\n",
    "        def distance(row: pd.Series) -> float:\n",
    "            diff = row - profile\n",
    "            diff = diff.fillna(diff.mean())\n",
    "            return float(np.sqrt((diff**2).sum()))\n",
    "\n",
    "        scores = matrix.apply(distance, axis=1).sort_values()\n",
    "        result = scores.to_frame(name=\"EuclideanDistance\")\n",
    "        result[\"Similarity (lower is better)\"] = result[\"EuclideanDistance\"]\n",
    "        return result\n",
    "\n",
    "\n",
    "class OpenAIInsightGenerator:\n",
    "    prompt_templates = {\n",
    "        \"question_basis\": (\n",
    "            \"You are a cultural analyst. Explain why communities scoring {score} on \"\n",
    "            \"'{question}'\\nfrom the {source} might interpret situations in that way. \"\n",
    "            \"Keep the answer under 250 words.\"\n",
    "        ),\n",
    "        \"situation_lens\": (\n",
    "            \"Given the cultural score(s) {scores} from the {source}, describe the \"\n",
    "            \"likely lens through\\nwhich a person would interpret the following \"\n",
    "            \"situation: {situation}. Limit to 250 words.\"\n",
    "        ),\n",
    "        \"score_delta\": (\n",
    "            \"Compare scores {score_a} and {score_b} from the {source}. Explain why \"\n",
    "            \"these perspectives might clash or align and provide actionable advice \"\n",
    "            \"in under 200 words.\"\n",
    "        ),\n",
    "        \"collaboration\": (\n",
    "            \"Two or more people have scores {scores} from the {source}. Provide \"\n",
    "            \"practical guidance on how they can work together effectively. 200 words \"\n",
    "            \"maximum.\"\n",
    "        ),\n",
    "        \"family\": (\n",
    "            \"Family members with cultural profile {scores} from the {source} are \"\n",
    "            \"seeking harmony. Offer strategies to improve their dynamics in under \"\n",
    "            \"200 words.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    def __init__(self, cache_path: Path):\n",
    "        self.cache_path = cache_path\n",
    "        try:\n",
    "            self.cache: Dict[str, str] = json.loads(cache_path.read_text(encoding=\"utf-8\"))\n",
    "        except json.JSONDecodeError:\n",
    "            self.cache = {}\n",
    "        self.client: Optional[OpenAI] = None\n",
    "        self.api_key: Optional[str] = None\n",
    "\n",
    "    def set_api_key(self, api_key: str) -> None:\n",
    "        if OpenAI is None:  # pragma: no cover - exercised only when dependency missing\n",
    "            raise ImportError(\"The openai package is not installed.\")\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def _cache_key(self, prompt: str) -> str:\n",
    "        return hashlib.sha256(prompt.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _render_prompt(self, template_key: str, **kwargs) -> str:\n",
    "        if template_key not in self.prompt_templates:\n",
    "            raise KeyError(f\"Unknown template '{template_key}'\")\n",
    "        prompt = self.prompt_templates[template_key].format(**kwargs)\n",
    "        prompt += \"\\n\\nDo not request or rely on personal data.\"\n",
    "        return prompt\n",
    "\n",
    "    def generate(self, template_key: str, **kwargs) -> str:\n",
    "        prompt = self._render_prompt(template_key, **kwargs)\n",
    "        cache_key = self._cache_key(prompt)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        if self.client is None:\n",
    "            return \"OpenAI client not configured. Set your API key before generating insights.\"\n",
    "        response = self.client.responses.create(  # pragma: no cover - network interaction\n",
    "            model=\"gpt-4o-mini\",\n",
    "            input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_output_tokens=500,\n",
    "        )\n",
    "        message = response.output_text.strip()\n",
    "        self.cache[cache_key] = message\n",
    "        self.cache_path.write_text(json.dumps(self.cache, indent=2), encoding=\"utf-8\")\n",
    "        return message\n",
    "\n",
    "\n",
    "class CultureExplorerService:\n",
    "    \"\"\"Backend helper used by the notebook and unit tests.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: CulturalDataset,\n",
    "        group_manager: GroupManager,\n",
    "        insight_generator: OpenAIInsightGenerator,\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.group_manager = group_manager\n",
    "        self.insight_generator = insight_generator\n",
    "\n",
    "    def get_countries(self) -> List[str]:\n",
    "        return self.dataset.get_countries()\n",
    "\n",
    "    def get_years(self) -> List[int]:\n",
    "        return self.dataset.get_years()\n",
    "\n",
    "    def get_group_questions(self) -> Dict[str, List[str]]:\n",
    "        return self.dataset.get_group_questions()\n",
    "    def get_score_matrix(\n",
    "        self, countries: List[str], year: int, group: Optional[str] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        return self.dataset.get_question_matrix(countries, year, group)\n",
    "\n",
    "\n",
    "    def ensure_location_available(self, location: str) -> None:\n",
    "        self.dataset.ensure_entity_available(location, \"location\")\n",
    "\n",
    "    def add_group_member(\n",
    "        self,\n",
    "        group_name: str,\n",
    "        member_name: str,\n",
    "        responses: Dict[QuestionKey, float],\n",
    "        group_type: str = \"team\",\n",
    "        reference_year: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        self.group_manager.add_member(\n",
    "            group_name,\n",
    "            member_name,\n",
    "            responses,\n",
    "            group_type=group_type,\n",
    "            reference_year=reference_year,\n",
    "        )\n",
    "\n",
    "    def match_group_to_country(\n",
    "        self, group_name: str, year: Optional[int] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        return self.group_manager.match_closest_country(group_name, year=year)\n",
    "\n",
    "\n",
    "def add_future_record(\n",
    "    dataset: CulturalDataset,\n",
    "    country: str,\n",
    "    iso3: str,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    year: int,\n",
    "    source: str,\n",
    "    question_group: str,\n",
    "    question: str,\n",
    "    score: float,\n",
    "    entity_type: str = \"country\",\n",
    ") -> None:\n",
    "    if not (0.0 <= score <= 1.0):\n",
    "        raise ValueError(\"Score must be between 0 and 1 inclusive.\")\n",
    "    record = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Country\": country,\n",
    "                \"ParentCountry\": country if entity_type == \"country\" else country,\n",
    "                \"ISO3\": iso3,\n",
    "                \"Latitude\": latitude,\n",
    "                \"Longitude\": longitude,\n",
    "                \"Year\": year,\n",
    "                \"Source\": source,\n",
    "                \"QuestionGroup\": question_group,\n",
    "                \"Question\": question,\n",
    "                \"QuestionCode\": dataset.get_question_code(question_group, question),\n",
    "                \"Score\": score,\n",
    "                \"EntityType\": entity_type,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    if entity_type == \"country\":\n",
    "        dataset.country_data = pd.concat([dataset.country_data, record], ignore_index=True)\n",
    "        dataset.data = dataset.country_data.copy()\n",
    "    elif entity_type == \"location\":\n",
    "        dataset.location_data = pd.concat([dataset.location_data, record], ignore_index=True)\n",
    "        dataset.location_lookup[country] = dataset.location_data[\n",
    "            dataset.location_data[\"Country\"] == country\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported entity type '{entity_type}'.\")\n",
    "    dataset._refresh_metadata()\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"CulturalDataset\",\n",
    "    \"GroupManager\",\n",
    "    \"OpenAIInsightGenerator\",\n",
    "    \"CultureExplorerService\",\n",
    "    \"add_future_record\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "_WORLD_URL = 'https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip'\n",
    "\n",
    "\n",
    "def _load_world() -> gpd.GeoDataFrame:\n",
    "    cache_dir = Path.home() / '.cache' / 'culture_explorer'\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    archive = cache_dir / 'ne_110m_admin_0_countries.zip'\n",
    "    if not archive.exists():\n",
    "        try:\n",
    "            import urllib.request\n",
    "\n",
    "            urllib.request.urlretrieve(_WORLD_URL, archive)\n",
    "        except Exception as exc:  # pragma: no cover - network failures\n",
    "            raise RuntimeError(\n",
    "                'Unable to download Natural Earth shapefile. '\n",
    "                'Please download the archive manually and place it at ' + str(archive)\n",
    "            ) from exc\n",
    "    return gpd.read_file(archive)\n",
    "\n",
    "\n",
    "_WORLD = None  # loaded lazily\n",
    "\n",
    "\n",
    "def _ensure_world() -> \"gpd.GeoDataFrame\":\n",
    "    if gpd is None:\n",
    "        raise ImportError(\"geopandas is required for map rendering.\")\n",
    "    global _WORLD\n",
    "    if _WORLD is None:\n",
    "        _WORLD = _load_world()\n",
    "    return _WORLD\n",
    "\n",
    "\n",
    "def _merge_world_scores(world: gpd.GeoDataFrame, scores: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    merged = world.merge(scores, left_on=\"ISO_A3\", right_on=\"ISO3\", how=\"left\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def render_static_culture_map(\n",
    "    dataset: CulturalDataset,\n",
    "    year: int,\n",
    "    question_group: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    include_locations: bool = True,\n",
    "    ax: Axes | None = None,\n",
    ") -> Tuple[Figure, Axes]:\n",
    "    \"\"\"Render a static matplotlib map with geopandas, free from JavaScript.\"\"\"\n",
    "\n",
    "    country_df = dataset.get_map_view(\"country\", year, question_group, question)\n",
    "    location_df = dataset.get_map_view(\"location\", year, question_group, question)\n",
    "\n",
    "    if country_df.empty and location_df.empty:\n",
    "        raise ValueError(\"No data available for the selected question and year.\")\n",
    "\n",
    "    base_scores = country_df[[\"ISO3\", \"Score\", \"Country\"]].copy()\n",
    "    world_scores = _merge_world_scores(_ensure_world(), base_scores)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6)) if ax is None else (ax.figure, ax)\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "    world_scores.plot(\n",
    "        column=\"Score\",\n",
    "        cmap=\"viridis\",\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"white\",\n",
    "        legend=True,\n",
    "        ax=ax,\n",
    "        norm=norm,\n",
    "        missing_kwds={\"color\": \"#f0f0f0\", \"edgecolor\": \"white\", \"label\": \"No data\"},\n",
    "    )\n",
    "\n",
    "    if include_locations and not location_df.empty:\n",
    "        sc = ax.scatter(\n",
    "            location_df[\"Longitude\"],\n",
    "            location_df[\"Latitude\"],\n",
    "            c=location_df[\"Score\"],\n",
    "            cmap=\"viridis\",\n",
    "            s=40,\n",
    "            norm=norm,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            alpha=0.8,\n",
    "            label=\"Local observations\",\n",
    "        )\n",
    "        if \"Local observations\" not in [t.get_text() for t in ax.legend_.texts] if ax.legend_ else True:\n",
    "            ax.legend(loc=\"lower left\")\n",
    "\n",
    "    ax.set_title(f\"{question} — {question_group} ({year})\", fontsize=14)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    if not country_df.empty:\n",
    "        top = country_df.nlargest(3, \"Score\")[[\"Country\", \"Score\"]]\n",
    "        bottom = country_df.nsmallest(3, \"Score\")[[\"Country\", \"Score\"]]\n",
    "        summary_lines = [\"Top countries:\"]\n",
    "        summary_lines.extend(\n",
    "            f\"  • {row.Country}: {row.Score:.2f}\" for row in top.itertuples()\n",
    "        )\n",
    "        summary_lines.append(\"Lowest countries:\")\n",
    "        summary_lines.extend(\n",
    "            f\"  • {row.Country}: {row.Score:.2f}\" for row in bottom.itertuples()\n",
    "        )\n",
    "        ax.text(\n",
    "            1.02,\n",
    "            0.5,\n",
    "            \"\\n\".join(summary_lines),\n",
    "            transform=ax.transAxes,\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "__all__ = [\"render_static_culture_map\"]\n",
    "QuestionKey = Tuple[str, str]\n",
    "\n",
    "\n",
    "class CulturalDataset:\n",
    "    \"\"\"Convenience wrapper around country and location survey records.\"\"\"\n",
    "\n",
    "    required_columns = {\n",
    "        \"Country\",\n",
    "        \"ParentCountry\",\n",
    "        \"ISO3\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Year\",\n",
    "        \"Source\",\n",
    "        \"QuestionGroup\",\n",
    "        \"Question\",\n",
    "        \"QuestionCode\",\n",
    "        \"Score\",\n",
    "        \"EntityType\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, country_df: pd.DataFrame, location_df: pd.DataFrame):\n",
    "        self._validate_input(\"country\", country_df)\n",
    "        self._validate_input(\"location\", location_df)\n",
    "        self.country_data = country_df.copy()\n",
    "        self.location_data = location_df.copy()\n",
    "        self.data = self.country_data.copy()\n",
    "        self.base_countries = sorted(self.country_data[\"Country\"].unique())\n",
    "        self.location_lookup = {\n",
    "            name: group.copy() for name, group in self.location_data.groupby(\"Country\")\n",
    "        }\n",
    "        self.question_lookup: Dict[QuestionKey, str] = {}\n",
    "        combined = pd.concat([self.country_data, self.location_data], ignore_index=True)\n",
    "        for _, row in (\n",
    "            combined[[\"QuestionGroup\", \"Question\", \"QuestionCode\"]]\n",
    "            .drop_duplicates()\n",
    "            .iterrows()\n",
    "        ):\n",
    "            self.question_lookup[(row[\"QuestionGroup\"], row[\"Question\"])] = row[\"QuestionCode\"]\n",
    "        self._refresh_metadata()\n",
    "\n",
    "    def _validate_input(self, label: str, frame: pd.DataFrame) -> None:\n",
    "        missing = self.required_columns - set(frame.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"{label.capitalize()} data is missing required columns: {missing}\")\n",
    "\n",
    "    def _refresh_metadata(self) -> None:\n",
    "        records: List[pd.DataFrame] = []\n",
    "        for frame in [self.country_data, self.location_data]:\n",
    "            if frame.empty:\n",
    "                continue\n",
    "            meta = (\n",
    "                frame.groupby(\"Country\")\n",
    "                .agg(\n",
    "                    Latitude=(\"Latitude\", \"mean\"),\n",
    "                    Longitude=(\"Longitude\", \"mean\"),\n",
    "                    ParentCountry=(\"ParentCountry\", \"first\"),\n",
    "                    EntityType=(\"EntityType\", \"first\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "            records.append(meta)\n",
    "        if records:\n",
    "            combined = (\n",
    "                pd.concat(records, ignore_index=True)\n",
    "                .drop_duplicates(\"Country\", keep=\"first\")\n",
    "            )\n",
    "            self.metadata = {row[\"Country\"]: row.to_dict() for _, row in combined.iterrows()}\n",
    "        else:\n",
    "            self.metadata = {}\n",
    "        self.available_entities = sorted(self.data[\"Country\"].unique())\n",
    "\n",
    "    def get_countries(self) -> List[str]:\n",
    "        return self.available_entities.copy()\n",
    "\n",
    "    def get_locations(self) -> List[str]:\n",
    "        return sorted(self.location_data[\"Country\"].unique())\n",
    "\n",
    "    def get_years(self) -> List[int]:\n",
    "        years = self.data[\"Year\"].unique().tolist()\n",
    "        return sorted(int(year) for year in years)\n",
    "\n",
    "    def get_group_questions(self) -> Dict[str, List[str]]:\n",
    "        grouped = self.data.groupby(\"QuestionGroup\")[\"Question\"].unique()\n",
    "        return {group: sorted(values.tolist()) for group, values in grouped.items()}\n",
    "\n",
    "    def get_sources(self) -> List[str]:\n",
    "        return sorted(self.data[\"Source\"].unique())\n",
    "\n",
    "    def get_question_code(self, group: str, question: str) -> str:\n",
    "        return self.question_lookup.get((group, question), question)\n",
    "\n",
    "    def filter_records(self, countries: Iterable[str], year: int) -> pd.DataFrame:\n",
    "        country_list = list(countries)\n",
    "        filtered = self.data[\n",
    "            self.data[\"Country\"].isin(country_list) & (self.data[\"Year\"] == year)\n",
    "        ]\n",
    "        if filtered.empty:\n",
    "            raise ValueError(\n",
    "                \"No records match the current filter. Try a different year or entity selection.\"\n",
    "            )\n",
    "        return filtered\n",
    "\n",
    "    def get_question_matrix(\n",
    "        self, countries: Iterable[str], year: int, group: Optional[str] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        subset = self.filter_records(countries, year)\n",
    "        if group and group != \"All Groups\":\n",
    "            subset = subset[subset[\"QuestionGroup\"] == group]\n",
    "        matrix = subset.pivot_table(index=\"Question\", columns=\"Country\", values=\"Score\")\n",
    "        return matrix.sort_index()\n",
    "\n",
    "    def compute_weighted_group_scores(\n",
    "        self, countries: Iterable[str], year: int, weights: Dict[QuestionKey, float]\n",
    "    ) -> pd.DataFrame:\n",
    "        subset = self.filter_records(countries, year).copy()\n",
    "        subset[\"Weight\"] = subset.apply(\n",
    "            lambda row: float(weights.get((row[\"QuestionGroup\"], row[\"Question\"]), 1.0)),\n",
    "            axis=1,\n",
    "        )\n",
    "        subset[\"WeightedScore\"] = subset[\"Score\"] * subset[\"Weight\"]\n",
    "\n",
    "        def safe_mean(group_df: pd.DataFrame) -> float:\n",
    "            weight_sum = group_df[\"Weight\"].sum()\n",
    "            if weight_sum == 0:\n",
    "                return float(\"nan\")\n",
    "            return group_df[\"WeightedScore\"].sum() / weight_sum\n",
    "\n",
    "        aggregated = (\n",
    "            subset.groupby([\"Country\", \"QuestionGroup\"], group_keys=False)\n",
    "            .apply(safe_mean, include_groups=False)\n",
    "            .unstack(\"QuestionGroup\")\n",
    "        )\n",
    "        return aggregated\n",
    "\n",
    "    def ensure_entity_available(self, entity_name: str, entity_type: str = \"country\") -> None:\n",
    "        if entity_name in self.available_entities:\n",
    "            return\n",
    "        if entity_type == \"location\":\n",
    "            if entity_name not in self.location_lookup:\n",
    "                raise ValueError(f\"Unknown location '{entity_name}'.\")\n",
    "            self.data = pd.concat([self.data, self.location_lookup[entity_name]], ignore_index=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown entity '{entity_name}'.\")\n",
    "        self._refresh_metadata()\n",
    "\n",
    "    def get_map_view(\n",
    "        self, entity_type: str, year: int, question_group: str, question: str\n",
    "    ) -> pd.DataFrame:\n",
    "        frame = self.country_data if entity_type == \"country\" else self.location_data\n",
    "        subset = frame[\n",
    "            (frame[\"Year\"] == year)\n",
    "            & (frame[\"QuestionGroup\"] == question_group)\n",
    "            & (frame[\"Question\"] == question)\n",
    "        ][[\n",
    "            \"Country\",\n",
    "            \"ParentCountry\",\n",
    "            \"ISO3\",\n",
    "            \"Latitude\",\n",
    "            \"Longitude\",\n",
    "            \"Year\",\n",
    "            \"Source\",\n",
    "            \"QuestionGroup\",\n",
    "            \"Question\",\n",
    "            \"Score\",\n",
    "            \"EntityType\",\n",
    "        ]]\n",
    "        return subset.reset_index(drop=True)\n",
    "\n",
    "    def get_entity_profile(\n",
    "        self, entity_name: str, entity_type: str, year: int\n",
    "    ) -> pd.DataFrame:\n",
    "        frame = self.country_data if entity_type == \"country\" else self.location_data\n",
    "        subset = frame[(frame[\"Country\"] == entity_name) & (frame[\"Year\"] == year)]\n",
    "        return subset[[\"QuestionGroup\", \"Question\", \"Score\"]]\n",
    "\n",
    "\n",
    "class GroupManager:\n",
    "    def __init__(self, dataset: CulturalDataset):\n",
    "        self.dataset = dataset\n",
    "        self.groups: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    def _validate_responses(self, responses: Dict[QuestionKey, float]) -> None:\n",
    "        valid_questions = self.dataset.get_group_questions()\n",
    "        for (group, question), value in responses.items():\n",
    "            if group not in valid_questions:\n",
    "                raise ValueError(f\"Unknown question group: {group}\")\n",
    "            if question not in valid_questions[group]:\n",
    "                raise ValueError(f\"Unknown question '{question}' for group '{group}'.\")\n",
    "            if not (0.0 <= value <= 1.0):\n",
    "                raise ValueError(\"Scores must be normalised between 0 and 1.\")\n",
    "\n",
    "    def add_member(\n",
    "        self,\n",
    "        group_name: str,\n",
    "        member_name: str,\n",
    "        responses: Dict[QuestionKey, float],\n",
    "        group_type: str = \"team\",\n",
    "        reference_year: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        group_name = group_name.strip()\n",
    "        if not group_name:\n",
    "            raise ValueError(\"Group name cannot be empty.\")\n",
    "        if not member_name.strip():\n",
    "            raise ValueError(\"Member name cannot be empty.\")\n",
    "        self._validate_responses(responses)\n",
    "        record = self.groups.setdefault(\n",
    "            group_name,\n",
    "            {\n",
    "                \"type\": group_type,\n",
    "                \"members\": {},\n",
    "                \"reference_year\": reference_year,\n",
    "            },\n",
    "        )\n",
    "        record[\"members\"][member_name] = responses\n",
    "        if reference_year is not None:\n",
    "            record[\"reference_year\"] = reference_year\n",
    "\n",
    "    def get_group_members(self, group_name: str) -> Dict[str, Dict[QuestionKey, float]]:\n",
    "        group = self.groups.get(group_name)\n",
    "        if not group:\n",
    "            raise KeyError(f\"Group '{group_name}' not found.\")\n",
    "        return group[\"members\"]  # type: ignore[return-value]\n",
    "\n",
    "    def compute_group_profile(self, group_name: str) -> pd.Series:\n",
    "        members = self.get_group_members(group_name)\n",
    "        if not members:\n",
    "            raise ValueError(\"Group has no members recorded yet.\")\n",
    "        df = pd.DataFrame(members).T\n",
    "        profile = df.mean(axis=0)\n",
    "        profile.name = group_name\n",
    "        return profile\n",
    "\n",
    "    def reference_year(self, group_name: str) -> Optional[int]:\n",
    "        group = self.groups.get(group_name)\n",
    "        return group.get(\"reference_year\") if group else None  # type: ignore[return-value]\n",
    "\n",
    "    def match_closest_country(\n",
    "        self, group_name: str, year: Optional[int] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        profile = self.compute_group_profile(group_name)\n",
    "        question_pairs = list(profile.index)\n",
    "        year = year or self.reference_year(group_name) or max(self.dataset.get_years())\n",
    "        entities = self.dataset.get_countries()\n",
    "        subset = self.dataset.filter_records(entities, year)\n",
    "        subset = subset[\n",
    "            subset.apply(\n",
    "                lambda row: (row[\"QuestionGroup\"], row[\"Question\"]) in question_pairs,\n",
    "                axis=1,\n",
    "            )\n",
    "        ]\n",
    "        matrix = subset.pivot_table(\n",
    "            index=\"Country\", columns=[\"QuestionGroup\", \"Question\"], values=\"Score\"\n",
    "        )\n",
    "\n",
    "        def distance(row: pd.Series) -> float:\n",
    "            diff = row - profile\n",
    "            diff = diff.fillna(diff.mean())\n",
    "            return float(np.sqrt((diff**2).sum()))\n",
    "\n",
    "        scores = matrix.apply(distance, axis=1).sort_values()\n",
    "        result = scores.to_frame(name=\"EuclideanDistance\")\n",
    "        result[\"Similarity (lower is better)\"] = result[\"EuclideanDistance\"]\n",
    "        return result\n",
    "\n",
    "\n",
    "class OpenAIInsightGenerator:\n",
    "    prompt_templates = {\n",
    "        \"question_basis\": (\n",
    "            \"You are a cultural analyst. Explain why communities scoring {score} on \"\n",
    "            \"'{question}'\\nfrom the {source} might interpret situations in that way. \"\n",
    "            \"Keep the answer under 250 words.\"\n",
    "        ),\n",
    "        \"situation_lens\": (\n",
    "            \"Given the cultural score(s) {scores} from the {source}, describe the \"\n",
    "            \"likely lens through\\nwhich a person would interpret the following \"\n",
    "            \"situation: {situation}. Limit to 250 words.\"\n",
    "        ),\n",
    "        \"score_delta\": (\n",
    "            \"Compare scores {score_a} and {score_b} from the {source}. Explain why \"\n",
    "            \"these perspectives might clash or align and provide actionable advice \"\n",
    "            \"in under 200 words.\"\n",
    "        ),\n",
    "        \"collaboration\": (\n",
    "            \"Two or more people have scores {scores} from the {source}. Provide \"\n",
    "            \"practical guidance on how they can work together effectively. 200 words \"\n",
    "            \"maximum.\"\n",
    "        ),\n",
    "        \"family\": (\n",
    "            \"Family members with cultural profile {scores} from the {source} are \"\n",
    "            \"seeking harmony. Offer strategies to improve their dynamics in under \"\n",
    "            \"200 words.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    def __init__(self, cache_path: Path):\n",
    "        self.cache_path = cache_path\n",
    "        try:\n",
    "            self.cache: Dict[str, str] = json.loads(cache_path.read_text(encoding=\"utf-8\"))\n",
    "        except json.JSONDecodeError:\n",
    "            self.cache = {}\n",
    "        self.client: Optional[OpenAI] = None\n",
    "        self.api_key: Optional[str] = None\n",
    "\n",
    "    def set_api_key(self, api_key: str) -> None:\n",
    "        if OpenAI is None:  # pragma: no cover - exercised only when dependency missing\n",
    "            raise ImportError(\"The openai package is not installed.\")\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def _cache_key(self, prompt: str) -> str:\n",
    "        return hashlib.sha256(prompt.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _render_prompt(self, template_key: str, **kwargs) -> str:\n",
    "        if template_key not in self.prompt_templates:\n",
    "            raise KeyError(f\"Unknown template '{template_key}'\")\n",
    "        prompt = self.prompt_templates[template_key].format(**kwargs)\n",
    "        prompt += \"\\n\\nDo not request or rely on personal data.\"\n",
    "        return prompt\n",
    "\n",
    "    def generate(self, template_key: str, **kwargs) -> str:\n",
    "        prompt = self._render_prompt(template_key, **kwargs)\n",
    "        cache_key = self._cache_key(prompt)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        if self.client is None:\n",
    "            return \"OpenAI client not configured. Set your API key before generating insights.\"\n",
    "        response = self.client.responses.create(  # pragma: no cover - network interaction\n",
    "            model=\"gpt-4o-mini\",\n",
    "            input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_output_tokens=500,\n",
    "        )\n",
    "        message = response.output_text.strip()\n",
    "        self.cache[cache_key] = message\n",
    "        self.cache_path.write_text(json.dumps(self.cache, indent=2), encoding=\"utf-8\")\n",
    "        return message\n",
    "\n",
    "\n",
    "class CultureExplorerService:\n",
    "    \"\"\"Backend helper used by the notebook and unit tests.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: CulturalDataset,\n",
    "        group_manager: GroupManager,\n",
    "        insight_generator: OpenAIInsightGenerator,\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.group_manager = group_manager\n",
    "        self.insight_generator = insight_generator\n",
    "\n",
    "    def get_countries(self) -> List[str]:\n",
    "        return self.dataset.get_countries()\n",
    "\n",
    "    def get_years(self) -> List[int]:\n",
    "        return self.dataset.get_years()\n",
    "\n",
    "    def get_group_questions(self) -> Dict[str, List[str]]:\n",
    "        return self.dataset.get_group_questions()\n",
    "\n",
    "    def ensure_location_available(self, location: str) -> None:\n",
    "        self.dataset.ensure_entity_available(location, \"location\")\n",
    "\n",
    "    def add_group_member(\n",
    "        self,\n",
    "        group_name: str,\n",
    "        member_name: str,\n",
    "        responses: Dict[QuestionKey, float],\n",
    "        group_type: str = \"team\",\n",
    "        reference_year: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        self.group_manager.add_member(\n",
    "            group_name,\n",
    "            member_name,\n",
    "            responses,\n",
    "            group_type=group_type,\n",
    "            reference_year=reference_year,\n",
    "        )\n",
    "\n",
    "    def match_group_to_country(\n",
    "        self, group_name: str, year: Optional[int] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        return self.group_manager.match_closest_country(group_name, year=year)\n",
    "\n",
    "\n",
    "def add_future_record(\n",
    "    dataset: CulturalDataset,\n",
    "    country: str,\n",
    "    iso3: str,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    year: int,\n",
    "    source: str,\n",
    "    question_group: str,\n",
    "    question: str,\n",
    "    score: float,\n",
    "    entity_type: str = \"country\",\n",
    ") -> None:\n",
    "    if not (0.0 <= score <= 1.0):\n",
    "        raise ValueError(\"Score must be between 0 and 1 inclusive.\")\n",
    "    record = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Country\": country,\n",
    "                \"ParentCountry\": country if entity_type == \"country\" else country,\n",
    "                \"ISO3\": iso3,\n",
    "                \"Latitude\": latitude,\n",
    "                \"Longitude\": longitude,\n",
    "                \"Year\": year,\n",
    "                \"Source\": source,\n",
    "                \"QuestionGroup\": question_group,\n",
    "                \"Question\": question,\n",
    "                \"QuestionCode\": dataset.get_question_code(question_group, question),\n",
    "                \"Score\": score,\n",
    "                \"EntityType\": entity_type,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    if entity_type == \"country\":\n",
    "        dataset.country_data = pd.concat([dataset.country_data, record], ignore_index=True)\n",
    "        dataset.data = dataset.country_data.copy()\n",
    "    elif entity_type == \"location\":\n",
    "        dataset.location_data = pd.concat([dataset.location_data, record], ignore_index=True)\n",
    "        dataset.location_lookup[country] = dataset.location_data[\n",
    "            dataset.location_data[\"Country\"] == country\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported entity type '{entity_type}'.\")\n",
    "    dataset._refresh_metadata()\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"CulturalDataset\",\n",
    "    \"GroupManager\",\n",
    "    \"OpenAIInsightGenerator\",\n",
    "    \"CultureExplorerService\",\n",
    "    \"add_future_record\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "_WORLD_URL = 'https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip'\n",
    "\n",
    "\n",
    "def _load_world() -> gpd.GeoDataFrame:\n",
    "    cache_dir = Path.home() / '.cache' / 'culture_explorer'\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    archive = cache_dir / 'ne_110m_admin_0_countries.zip'\n",
    "    if not archive.exists():\n",
    "        try:\n",
    "            import urllib.request\n",
    "\n",
    "            urllib.request.urlretrieve(_WORLD_URL, archive)\n",
    "        except Exception as exc:  # pragma: no cover - network failures\n",
    "            raise RuntimeError(\n",
    "                'Unable to download Natural Earth shapefile. '\n",
    "                'Please download the archive manually and place it at ' + str(archive)\n",
    "            ) from exc\n",
    "    return gpd.read_file(archive)\n",
    "\n",
    "\n",
    "_WORLD = None  # loaded lazily\n",
    "\n",
    "\n",
    "def _merge_world_scores(world: gpd.GeoDataFrame, scores: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    merged = world.merge(scores, left_on=\"ISO_A3\", right_on=\"ISO3\", how=\"left\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def render_static_culture_map(\n",
    "    dataset: CulturalDataset,\n",
    "    year: int,\n",
    "    question_group: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    include_locations: bool = True,\n",
    "    ax: Axes | None = None,\n",
    ") -> Tuple[Figure, Axes]:\n",
    "    \"\"\"Render a static matplotlib map with geopandas, free from JavaScript.\"\"\"\n",
    "\n",
    "    country_df = dataset.get_map_view(\"country\", year, question_group, question)\n",
    "    location_df = dataset.get_map_view(\"location\", year, question_group, question)\n",
    "\n",
    "    if country_df.empty and location_df.empty:\n",
    "        raise ValueError(\"No data available for the selected question and year.\")\n",
    "\n",
    "    base_scores = country_df[[\"ISO3\", \"Score\", \"Country\"]].copy()\n",
    "    world_scores = _merge_world_scores(_ensure_world(), base_scores)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6)) if ax is None else (ax.figure, ax)\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "    world_scores.plot(\n",
    "        column=\"Score\",\n",
    "        cmap=\"viridis\",\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"white\",\n",
    "        legend=True,\n",
    "        ax=ax,\n",
    "        norm=norm,\n",
    "        missing_kwds={\"color\": \"#f0f0f0\", \"edgecolor\": \"white\", \"label\": \"No data\"},\n",
    "    )\n",
    "\n",
    "    if include_locations and not location_df.empty:\n",
    "        sc = ax.scatter(\n",
    "            location_df[\"Longitude\"],\n",
    "            location_df[\"Latitude\"],\n",
    "            c=location_df[\"Score\"],\n",
    "            cmap=\"viridis\",\n",
    "            s=40,\n",
    "            norm=norm,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            alpha=0.8,\n",
    "            label=\"Local observations\",\n",
    "        )\n",
    "        if \"Local observations\" not in [t.get_text() for t in ax.legend_.texts] if ax.legend_ else True:\n",
    "            ax.legend(loc=\"lower left\")\n",
    "\n",
    "    ax.set_title(f\"{question} — {question_group} ({year})\", fontsize=14)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    if not country_df.empty:\n",
    "        top = country_df.nlargest(3, \"Score\")[[\"Country\", \"Score\"]]\n",
    "        bottom = country_df.nsmallest(3, \"Score\")[[\"Country\", \"Score\"]]\n",
    "        summary_lines = [\"Top countries:\"]\n",
    "        summary_lines.extend(\n",
    "            f\"  • {row.Country}: {row.Score:.2f}\" for row in top.itertuples()\n",
    "        )\n",
    "        summary_lines.append(\"Lowest countries:\")\n",
    "        summary_lines.extend(\n",
    "            f\"  • {row.Country}: {row.Score:.2f}\" for row in bottom.itertuples()\n",
    "        )\n",
    "        ax.text(\n",
    "            1.02,\n",
    "            0.5,\n",
    "            \"\\n\".join(summary_lines),\n",
    "            transform=ax.transAxes,\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "__all__ = [\"render_static_culture_map\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968a007",
   "metadata": {},
   "source": [
    "## 3. Data management utilities\n",
    "\n",
    "The `CulturalDataset` orchestrates filtering, aggregation, and persistence. `GroupManager` stores survey responses for family/team groups. `OpenAIInsightGenerator` wraps the OpenAI Responses API with a lightweight caching layer that prevents repeated prompts from consuming tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d712712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:15.650792Z",
     "iopub.status.busy": "2025-10-02T16:16:15.650352Z",
     "iopub.status.idle": "2025-10-02T16:16:15.704485Z",
     "shell.execute_reply": "2025-10-02T16:16:15.703116Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = CulturalDataset(country_records, location_records)\n",
    "group_manager = GroupManager(dataset)\n",
    "insight_generator = OpenAIInsightGenerator(CACHE_PATH)\n",
    "service = CultureExplorerService(dataset, group_manager, insight_generator)\n",
    "display(\n",
    "    Markdown(\n",
    "        \"* Comparison-ready entities: \"\n",
    "        + \", \".join(dataset.get_countries())\n",
    "        + \"\\n* Local areas available: \"\n",
    "        + (\", \".join(dataset.get_locations()) or \"None yet\")\n",
    "        + \"\\n* Years available: \"\n",
    "        + \", \".join(map(str, dataset.get_years()))\n",
    "        + \"\\n* Survey sources: \"\n",
    "        + \", \".join(dataset.get_sources())\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e07c6d",
   "metadata": {},
   "source": [
    "## 4. Static cultural map (geopandas & matplotlib)\n",
    "\n",
    "Render a publication-ready, fully Pythonic map showing national and local observations without relying on JavaScript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342b068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:15.708819Z",
     "iopub.status.busy": "2025-10-02T16:16:15.708225Z",
     "iopub.status.idle": "2025-10-02T16:16:15.759594Z",
     "shell.execute_reply": "2025-10-02T16:16:15.758572Z"
    }
   },
   "outputs": [],
   "source": [
    "def render_culture_map(\n",
    "    year: int,\n",
    "    question_group: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    include_locations: bool = True,\n",
    ") -> None:\n",
    "    year = int(year)\n",
    "    fig, ax = render_static_culture_map(\n",
    "        dataset,\n",
    "        year,\n",
    "        question_group,\n",
    "        question,\n",
    "        include_locations=include_locations,\n",
    "    )\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "    country_subset = dataset.get_map_view(\"country\", year, question_group, question)\n",
    "    if not country_subset.empty:\n",
    "        display(\n",
    "            country_subset\n",
    "            .sort_values(\"Score\", ascending=False)[[\"Country\", \"Score\", \"Source\"]]\n",
    "            .reset_index(drop=True)\n",
    "            .head(10)\n",
    "        )\n",
    "    location_subset = dataset.get_map_view(\"location\", year, question_group, question)\n",
    "    if include_locations and not location_subset.empty:\n",
    "        display(\n",
    "            location_subset\n",
    "            .sort_values(\"Score\", ascending=False)[[\"Country\", \"ParentCountry\", \"Score\"]]\n",
    "            .reset_index(drop=True)\n",
    "            .head(10)\n",
    "        )\n",
    "\n",
    "\n",
    "def display_default_map() -> None:\n",
    "    default_year = max(dataset.get_years())\n",
    "    default_group = next(iter(dataset.get_group_questions().keys()))\n",
    "    default_question = dataset.get_group_questions()[default_group][0]\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"Static map for **{default_question}** ({default_group}, {default_year}). \"\n",
    "            \"Local observations are plotted as circles.\"\n",
    "        )\n",
    "    )\n",
    "    render_culture_map(default_year, default_group, default_question)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_default_map()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865d713",
   "metadata": {},
   "source": [
    "## 5. Comparison matrix with adjustable weights\n",
    "\n",
    "Use the widget below to select any combination of countries and tune the relative importance of each survey question. The accordion exposes per-question weights while the dashboard summarises question-level and group-level standings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8dfe07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:15.763491Z",
     "iopub.status.busy": "2025-10-02T16:16:15.763134Z",
     "iopub.status.idle": "2025-10-02T16:16:16.373860Z",
     "shell.execute_reply": "2025-10-02T16:16:16.372253Z"
    }
   },
   "outputs": [],
   "source": [
    "if widgets is None:\n",
    "    display(Markdown('ipywidgets is not available; interactive dashboards are disabled.'))\n",
    "else:\n",
    "    class ComparisonDashboard:\n",
    "        def __init__(self, dataset: CulturalDataset):\n",
    "            self.dataset = dataset\n",
    "            self.weight_controls: Dict[str, Dict[str, widgets.FloatSlider]] = {}\n",
    "    \n",
    "            self.country_select = widgets.SelectMultiple(\n",
    "                options=dataset.get_countries(),\n",
    "                value=tuple(dataset.get_countries()[:3]),\n",
    "                description='Countries',\n",
    "                layout=widgets.Layout(width='300px', height='200px'),\n",
    "            )\n",
    "            self.year_dropdown = widgets.Dropdown(\n",
    "                options=dataset.get_years(),\n",
    "                value=max(dataset.get_years()),\n",
    "                description='Year',\n",
    "            )\n",
    "            group_options = ['All Groups'] + list(dataset.get_group_questions().keys())\n",
    "            self.group_dropdown = widgets.Dropdown(\n",
    "                options=group_options,\n",
    "                value='All Groups',\n",
    "                description='Focus',\n",
    "            )\n",
    "            self.output_table = widgets.Output()\n",
    "            self.output_heatmap = widgets.Output()\n",
    "    \n",
    "            weight_children = []\n",
    "            for group, questions in dataset.get_group_questions().items():\n",
    "                sliders = []\n",
    "                controls = {}\n",
    "                for question in questions:\n",
    "                    slider = widgets.FloatSlider(\n",
    "                        value=1.0,\n",
    "                        min=0.0,\n",
    "                        max=2.0,\n",
    "                        step=0.05,\n",
    "                        description=question[:22] + ('…' if len(question) > 22 else ''),\n",
    "                        readout=True,\n",
    "                        readout_format='.2f',\n",
    "                        style={'description_width': 'initial'},\n",
    "                    )\n",
    "                    sliders.append(slider)\n",
    "                    controls[question] = slider\n",
    "                self.weight_controls[group] = controls\n",
    "                weight_children.append(widgets.VBox(sliders))\n",
    "            self.weight_accordion = widgets.Accordion(children=weight_children)\n",
    "            for idx, group in enumerate(dataset.get_group_questions().keys()):\n",
    "                self.weight_accordion.set_title(idx, group)\n",
    "    \n",
    "            self.country_select.observe(self._update_dashboard, names='value')\n",
    "            self.year_dropdown.observe(self._update_dashboard, names='value')\n",
    "            self.group_dropdown.observe(self._update_dashboard, names='value')\n",
    "            for controls in self.weight_controls.values():\n",
    "                for slider in controls.values():\n",
    "                    slider.observe(self._update_dashboard, names='value')\n",
    "    \n",
    "            self.container = widgets.VBox([\n",
    "                widgets.HBox([self.country_select, widgets.VBox([self.year_dropdown, self.group_dropdown])]),\n",
    "                self.weight_accordion,\n",
    "                self.output_table,\n",
    "                self.output_heatmap,\n",
    "            ])\n",
    "            self._update_dashboard()\n",
    "    \n",
    "        def _collect_weights(self) -> Dict[Tuple[str, str], float]:\n",
    "            weights = {}\n",
    "            for group, controls in self.weight_controls.items():\n",
    "                for question, slider in controls.items():\n",
    "                    weights[(group, question)] = slider.value\n",
    "            return weights\n",
    "    \n",
    "        def _update_dashboard(self, *_):\n",
    "            if not self.country_select.value:\n",
    "                return\n",
    "            countries = list(self.country_select.value)\n",
    "            year = self.year_dropdown.value\n",
    "            group_focus = self.group_dropdown.value\n",
    "            weights = self._collect_weights()\n",
    "    \n",
    "            with self.output_table:\n",
    "                clear_output(wait=True)\n",
    "                try:\n",
    "                    matrix = self.dataset.get_question_matrix(countries, year, group_focus)\n",
    "                    display(Markdown(\"### Question-level comparison\"))\n",
    "                    display(matrix.round(3))\n",
    "                    if group_focus != 'All Groups':\n",
    "                        focus_weights = {q: weights[(group_focus, q)] for q in matrix.index}\n",
    "                        summary = (\n",
    "                            matrix.mul(pd.Series(focus_weights), axis=0).sum(axis=0) /\n",
    "                            max(sum(focus_weights.values()), 1e-9)\n",
    "                        )\n",
    "                        display(Markdown(\"**Weighted group score (per country):**\"))\n",
    "                        display(summary.round(3).to_frame(name='Score'))\n",
    "                    else:\n",
    "                        aggregated = self.dataset.compute_weighted_group_scores(countries, year, weights)\n",
    "                        display(Markdown(\"### Group-level weighted scores\"))\n",
    "                        display(aggregated.round(3))\n",
    "                except Exception as exc:\n",
    "                    display(Markdown(f\"**Error:** {exc}\"))\n",
    "    \n",
    "            with self.output_heatmap:\n",
    "                clear_output(wait=True)\n",
    "                try:\n",
    "                    aggregated = self.dataset.compute_weighted_group_scores(countries, year, weights).fillna(np.nan)\n",
    "                    fig, ax = plt.subplots(figsize=(8, max(3, len(aggregated) * 0.6)))\n",
    "                    data = np.ma.masked_invalid(aggregated.values.astype(float))\n",
    "                    im = ax.imshow(data, aspect=\"auto\", cmap=\"viridis\", vmin=0, vmax=1)\n",
    "                    ax.set_xticks(range(len(aggregated.columns)))\n",
    "                    ax.set_xticklabels(list(aggregated.columns), rotation=45, ha=\"right\")\n",
    "                    ax.set_yticks(range(len(aggregated.index)))\n",
    "                    ax.set_yticklabels(list(aggregated.index))\n",
    "                    ax.set_xlabel(\"Question Group\")\n",
    "                    ax.set_ylabel(\"Country\")\n",
    "                    ax.set_title(\"Weighted cultural proximity\")\n",
    "                    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"Score\")\n",
    "                    fig.tight_layout()\n",
    "                    display(fig)\n",
    "                    plt.close(fig)\n",
    "                except Exception as exc:\n",
    "                    display(Markdown(f\"**Unable to render heatmap:** {exc}\"))\n",
    "        def add_from_map(self, entity_name: str, entity_type: str = \"country\") -> None:\n",
    "            try:\n",
    "                self.dataset.ensure_entity_available(entity_name, entity_type)\n",
    "            except ValueError as exc:\n",
    "                with self.output_table:\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(f\"**Map selection error:** {exc}\"))\n",
    "                return\n",
    "            options = list(self.country_select.options)\n",
    "            if entity_name not in options:\n",
    "                options.append(entity_name)\n",
    "                options.sort(key=str.lower)\n",
    "                self.country_select.options = options\n",
    "            current = list(self.country_select.value)\n",
    "            if entity_name not in current:\n",
    "                current.append(entity_name)\n",
    "                self.country_select.value = tuple(current)\n",
    "            self._update_dashboard()\n",
    "    \n",
    "        def display(self):\n",
    "            display(self.container)\n",
    "    \n",
    "    comparison_dashboard = ComparisonDashboard(dataset)\n",
    "    comparison_dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db254273",
   "metadata": {},
   "source": [
    "## 6. Family and team survey capture\n",
    "\n",
    "Record member scores (normalised 0–1) for a family or team group below. After adding members you can finalise the profile to discover the nearest national culture using Euclidean similarity across the shared question space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013ad3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:16.402980Z",
     "iopub.status.busy": "2025-10-02T16:16:16.402683Z",
     "iopub.status.idle": "2025-10-02T16:16:16.501943Z",
     "shell.execute_reply": "2025-10-02T16:16:16.500125Z"
    }
   },
   "outputs": [],
   "source": [
    "if widgets is None:\n",
    "    display(Markdown('ipywidgets is not available; group survey widget is disabled.'))\n",
    "else:\n",
    "    class GroupSurveyWidget:\n",
    "        def __init__(self, dataset: CulturalDataset, manager: GroupManager):\n",
    "            self.dataset = dataset\n",
    "            self.manager = manager\n",
    "            self.output = widgets.Output()\n",
    "            self._build_widgets()\n",
    "    \n",
    "        def _build_widgets(self):\n",
    "            self.group_type = widgets.ToggleButtons(\n",
    "                options=[('Family', 'family'), ('Team', 'team')],\n",
    "                description='Group type',\n",
    "            )\n",
    "            self.group_name = widgets.Text(description='Group name', placeholder='e.g. Rivera Family')\n",
    "            self.member_name = widgets.Text(description='Member name', placeholder='e.g. Ana')\n",
    "            self.year_dropdown = widgets.Dropdown(\n",
    "                options=self.dataset.get_years(),\n",
    "                value=max(self.dataset.get_years()),\n",
    "                description='Ref. year',\n",
    "            )\n",
    "            self.question_sliders: Dict[Tuple[str, str], widgets.FloatSlider] = {}\n",
    "            slider_boxes = []\n",
    "            for group, questions in self.dataset.get_group_questions().items():\n",
    "                sliders = []\n",
    "                for question in questions:\n",
    "                    slider = widgets.FloatSlider(\n",
    "                        value=0.5,\n",
    "                        min=0.0,\n",
    "                        max=1.0,\n",
    "                        step=0.01,\n",
    "                        description=question[:28] + ('…' if len(question) > 28 else ''),\n",
    "                        style={'description_width': 'initial'},\n",
    "                        readout=True,\n",
    "                        readout_format='.2f',\n",
    "                    )\n",
    "                    self.question_sliders[(group, question)] = slider\n",
    "                    sliders.append(slider)\n",
    "                slider_boxes.append(widgets.VBox([widgets.HTML(f\"<h4>{group}</h4>\")] + sliders))\n",
    "            self.slider_accordion = widgets.Accordion(children=slider_boxes)\n",
    "            for idx, group in enumerate(self.dataset.get_group_questions().keys()):\n",
    "                self.slider_accordion.set_title(idx, group)\n",
    "    \n",
    "            self.add_member_button = widgets.Button(description='Add member responses', button_style='success')\n",
    "            self.finalise_button = widgets.Button(description='Finalise & match', button_style='primary')\n",
    "            self.add_member_button.on_click(self._handle_add_member)\n",
    "            self.finalise_button.on_click(self._handle_finalise)\n",
    "    \n",
    "            self.form = widgets.VBox([\n",
    "                widgets.HBox([self.group_type, self.group_name, self.member_name, self.year_dropdown]),\n",
    "                self.slider_accordion,\n",
    "                widgets.HBox([self.add_member_button, self.finalise_button]),\n",
    "                self.output,\n",
    "            ])\n",
    "    \n",
    "        def _collect_scores(self) -> Dict[Tuple[str, str], float]:\n",
    "            return {key: slider.value for key, slider in self.question_sliders.items()}\n",
    "    \n",
    "        def _handle_add_member(self, _):\n",
    "            try:\n",
    "                responses = self._collect_scores()\n",
    "                self.manager.add_member(\n",
    "                    group_name=self.group_name.value,\n",
    "                    member_name=self.member_name.value,\n",
    "                    responses=responses,\n",
    "                    group_type=self.group_type.value,\n",
    "                    reference_year=self.year_dropdown.value,\n",
    "                )\n",
    "                with self.output:\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(f\"✅ Added survey for **{self.member_name.value}**.\"))\n",
    "                self.member_name.value = ''\n",
    "            except Exception as exc:\n",
    "                with self.output:\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(f\"⚠️ {exc}\"))\n",
    "    \n",
    "        def _handle_finalise(self, _):\n",
    "            try:\n",
    "                group_name = self.group_name.value\n",
    "                profile = self.manager.compute_group_profile(group_name)\n",
    "                match_df = self.manager.match_closest_country(group_name)\n",
    "                with self.output:\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(f\"### Profile for **{group_name}**\"))\n",
    "                    display(profile.round(3).to_frame(name='Average score'))\n",
    "                    display(Markdown(\"### Closest national cultures\"))\n",
    "                    display(match_df.head(5).round(3))\n",
    "            except Exception as exc:\n",
    "                with self.output:\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(f\"⚠️ {exc}\"))\n",
    "    \n",
    "        def display(self):\n",
    "            display(self.form)\n",
    "    \n",
    "    group_widget = GroupSurveyWidget(dataset, group_manager)\n",
    "    group_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c961b8",
   "metadata": {},
   "source": [
    "## 7. Pure Python backend helpers\n",
    "The previous FastAPI layer has been replaced with lightweight utility functions so that everything runs natively within this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8bb839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:16.525399Z",
     "iopub.status.busy": "2025-10-02T16:16:16.524900Z",
     "iopub.status.idle": "2025-10-02T16:16:16.558368Z",
     "shell.execute_reply": "2025-10-02T16:16:16.557244Z"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        \"Pure Python helper `service` initialised. Use its methods to explore \"\n",
    "        \"and manipulate the dataset without running a web server.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af10943",
   "metadata": {},
   "source": [
    "## 8. Backend helper verification\n",
    "Quick smoke-tests ensure the lightweight service behaves as expected when called directly from Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e9a7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:16.562272Z",
     "iopub.status.busy": "2025-10-02T16:16:16.561847Z",
     "iopub.status.idle": "2025-10-02T16:16:16.661492Z",
     "shell.execute_reply": "2025-10-02T16:16:16.660123Z"
    }
   },
   "outputs": [],
   "source": [
    "latest_year = max(service.get_years())\n",
    "sample_countries = service.get_countries()[:2]\n",
    "matrix = dataset.get_question_matrix(sample_countries, latest_year)\n",
    "\n",
    "display(Markdown(\n",
    "    f\"Validated helper service with {len(service.get_countries())} countries and {len(service.get_years())} years of data.\"\n",
    "))\n",
    "display(matrix.head())\n",
    "\n",
    "\n",
    "# Additional manipulations can be performed via the dataset and group_manager objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a2bef",
   "metadata": {},
   "source": [
    "## 9. OpenAI cultural insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da45f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:16.665523Z",
     "iopub.status.busy": "2025-10-02T16:16:16.665119Z",
     "iopub.status.idle": "2025-10-02T16:16:16.673638Z",
     "shell.execute_reply": "2025-10-02T16:16:16.672293Z"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown('Configure an OpenAI API key to generate insights via the notebook.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d716bc",
   "metadata": {},
   "source": [
    "## 10. Adding retrospective or future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0f4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T16:16:16.677353Z",
     "iopub.status.busy": "2025-10-02T16:16:16.676888Z",
     "iopub.status.idle": "2025-10-02T16:16:16.699604Z",
     "shell.execute_reply": "2025-10-02T16:16:16.698251Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_future_record(\n",
    "    country: str,\n",
    "    iso3: str,\n",
    "    latitude: float,\n",
    "    longitude: float,\n",
    "    year: int,\n",
    "    source: str,\n",
    "    question_group: str,\n",
    "    question: str,\n",
    "    score: float,\n",
    "    entity_type: str = 'country',\n",
    "    parent_country: Optional[str] = None,\n",
    ") -> None:\n",
    "    question_code = dataset.get_question_code(question_group, question)\n",
    "    record = pd.DataFrame([\n",
    "        {\n",
    "            'Country': country,\n",
    "            'ISO3': iso3,\n",
    "            'Latitude': latitude,\n",
    "            'Longitude': longitude,\n",
    "            'Year': year,\n",
    "            'Source': source,\n",
    "            'QuestionGroup': question_group,\n",
    "            'Question': question,\n",
    "            'QuestionCode': question_code,\n",
    "            'Score': score,\n",
    "            'EntityType': entity_type,\n",
    "            'ParentCountry': parent_country or country,\n",
    "        }\n",
    "    ])\n",
    "    if entity_type == 'country':\n",
    "        dataset.country_data = pd.concat([dataset.country_data, record], ignore_index=True)\n",
    "        dataset.data = dataset.country_data.copy()\n",
    "    elif entity_type == 'location':\n",
    "        dataset.location_data = pd.concat([dataset.location_data, record], ignore_index=True)\n",
    "        dataset.location_lookup[country] = dataset.location_data[dataset.location_data['Country'] == country]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported entity type '{entity_type}'.\")\n",
    "    dataset._refresh_metadata()\n",
    "\n",
    "\n",
    "add_future_record(\n",
    "    country='Futuria',\n",
    "    iso3='FTR',\n",
    "    latitude=12.34,\n",
    "    longitude=56.78,\n",
    "    year=2025,\n",
    "    source='EVS',\n",
    "    question_group=list(dataset.get_group_questions().keys())[0],\n",
    "    question=dataset.get_group_questions()[list(dataset.get_group_questions().keys())[0]][0],\n",
    "    score=0.67,\n",
    ")\n",
    "display(Markdown(\"Appended a placeholder future data point for Futuria (2025).\"))\n",
    "dataset.data = dataset.data[dataset.data['Country'] != 'Futuria']\n",
    "dataset.country_data = dataset.country_data[dataset.country_data['Country'] != 'Futuria']\n",
    "dataset._refresh_metadata()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
